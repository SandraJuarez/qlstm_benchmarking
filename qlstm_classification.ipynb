{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de PennyLane: 0.35.1\n"
     ]
    }
   ],
   "source": [
    "pennylane_version = qml.__version__\n",
    "print(f\"Versión de PennyLane: {pennylane_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits,\n",
    "                n_qlayers=1,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit.torch\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size #features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "\n",
    "        #self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        #self.dev = qml.device('qiskit.basicaer', wires=self.n_qubits)\n",
    "        #self.dev = qml.device('qiskit.ibm', wires=self.n_qubits)\n",
    "        # use 'qiskit.ibmq' instead to run on hardware\n",
    "\n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget,torch_device='cuda')\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input,torch_device='cuda')\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update,torch_device='cuda')\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output,torch_device='cuda')\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_forget)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\",diff_method='backprop')\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_input)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\",diff_method='backprop')\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_update)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\",diff_method='backprop')\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_output)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\",diff_method='backprop')\n",
    "\n",
    "        weight_shapesf = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        weight_shapesi = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        weight_shapesu = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        weight_shapeso = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        device = torch.device('cuda')\n",
    "        \n",
    "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapesf),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapesi),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapesu),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapeso)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        device = torch.device('cuda')\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).to(device)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size).to(device)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :] #x has shape (batch,seq_len,features)\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t) #it has size (batch_size, hidden)\n",
    "            hidden_seq.append(h_t.unsqueeze(0)) #we will end with a number of sequences of the size of the window of time \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0) #(window, batch_size,hidden)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous() #(batch_size,window,hidden)\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYBRID CIRCUIT\n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HLstm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_qubits, target_size,  backend='default.qubit.torch'):\n",
    "        super(HLstm,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_qubits=n_qubits\n",
    "        self.lstm = QLSTM(self.input_size, self.hidden_dim, self.n_qubits, backend=backend)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "    def forward(self, sequence):\n",
    "        \n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        #print('el shape de lstm_out',lstm_out.shape)\n",
    "        #target = self.hidden2tag(lstm_out.view(len(sequence), -1))\n",
    "        target = self.hidden2tag(lstm_out[:, -1, :]) #we take the last h_t. It has shape (batch_size,hidden). We map it to (batch_size,features)\n",
    "        #print('el shape del target',target.shape)\n",
    "        #tag_scores = F.log_softmax(tag_logits, dim=1)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic architectureee (only to verify which dimensions we should have)\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la red LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size,dtype=torch.float).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size,dtype=torch.float).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        #print('el shape de classic lstm_out',out.shape)\n",
    "        out = self.fc(out[:, -1, :])  # Tomar la última salida de la secuencia\n",
    "        #print('el shape de classic lstm_out luego dee self.fc(out[:, -1, :]',out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_model_summary import summary\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#from qlstm_pennylane import QLSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el device es cuda\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay una GPU disponible y establecer el dispositivo\n",
    "device = torch.device('cuda')\n",
    "print('el device es',device)\n",
    "df = pd.read_csv('mitbih_train.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = df.iloc[:, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "data=data[0:5000,:]\n",
    "sequences = []\n",
    "for i in range(len(data)):\n",
    "    seq = data[i,0:50]\n",
    "    label = data[i,-1]\n",
    "    if i==0:\n",
    "        print(label)\n",
    "    sequences.append((seq, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99366087, 0.80190176, 0.19334389, 0.        , 0.2155309 ,\n",
       "        0.21394612, 0.22187005, 0.23137876, 0.23137876, 0.22503962,\n",
       "        0.22503962, 0.23454833, 0.23771791, 0.23454833, 0.23771791,\n",
       "        0.24247226, 0.25515056, 0.25515056, 0.25515056, 0.26148969,\n",
       "        0.27258319, 0.27258319, 0.27258319, 0.28526148, 0.29001585,\n",
       "        0.29477021, 0.29477021, 0.30269414, 0.31061807, 0.30903327,\n",
       "        0.30427891, 0.31537244, 0.32171157, 0.32329637, 0.33438987,\n",
       "        0.34389856, 0.35816166, 0.35023773, 0.3312203 , 0.318542  ,\n",
       "        0.31220284, 0.29635498, 0.27575278, 0.26782885, 0.27258319,\n",
       "        0.27099842, 0.26148969, 0.26148969, 0.26941362, 0.26307449]),\n",
       " 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(sequences))\n",
    "train_data = sequences[:train_size]\n",
    "test_data = sequences[train_size:]\n",
    "\n",
    "# Crear DataLoader para cargar los datos\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader, num_epochs, learning_rate):\n",
    "    # Función de pérdida y optimizador\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_list = []\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    for epoch in range(num_epochs):\n",
    "        for seq, label in train_loader:\n",
    "            seq = seq.float()\n",
    "            seq = seq.unsqueeze(-1)\n",
    "            label = label.long()\n",
    "            #label = label.unsqueeze(-1)\n",
    "            #print(label)\n",
    "            # Mover las secuencias y etiquetas a la GPU\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "           # print('el shape de seq es',seq.shape)\n",
    "            #print('el shape de labeel es',label.shape)\n",
    "            outputs = model(seq)\n",
    "            #print('outputs',outputs.shape)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.11f}')\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de puntos que estamos empleando es: 4000\n",
      "weight_shapes = (n_qlayers, n_qubits) = (1, 5)\n"
     ]
    }
   ],
   "source": [
    "# Hiperparámetros\n",
    "sequence_length = 50 # Longitud de la secuencia de entrada\n",
    "input_size = 1       # Features\n",
    "hidden_size = 74     # Tamaño de la capa oculta LSTM\n",
    "qubits=5             #\n",
    "num_layers = 1      # Número de capas LSTM\n",
    "output_size = 5      # clases\n",
    "learning_rate = 0.02\n",
    "num_epochs = 1\n",
    "#seed \n",
    "torch.manual_seed(0)\n",
    "#(self, seq_len, hidden_dim,  target_size=5, n_qubits=4, backend='default.qubit'):\n",
    "#self, input_size, hidden_size, num_layers, output_size\n",
    "#model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "print('La cantidad de puntos que estamos empleando es:',train_size)\n",
    "model = HLstm(input_size,hidden_size,qubits,output_size)\n",
    "model.to(device)\n",
    "loss_list=train_model(model,train_loader, num_epochs, learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.axis(ymin=0,ymax=0.00005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,name):\n",
    "    #guardar el modelo\n",
    "    torch.save(model.state_dict(), 'weights/'+ name + '.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5555227994918823]\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model,'cuda_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data):\n",
    "    # Poner el modelo en modo de evaluación\n",
    "    model.eval()\n",
    "    # Determinar el dispositivo (CPU o GPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Mover el modelo al dispositivo\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_predictions = []\n",
    "        for data_batch in test_data:\n",
    "            test_seq = torch.FloatTensor(data_batch[0]).unsqueeze(dim=0).to(device)\n",
    "            test_seq = test_seq.unsqueeze(-1)\n",
    "            prediction = torch.max(model(test_seq),1)\n",
    "            \n",
    "            test_predictions.append(prediction)\n",
    "    \n",
    "    # Concatenar las predicciones y retornar\n",
    "    return torch.cat(test_predictions, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions= predict(model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions.cpu()\n",
    "test_predictions_np = test_predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0563802 ]\n",
      " [0.05623598]\n",
      " [0.0561969 ]\n",
      " [0.05645838]\n",
      " [0.05622224]\n",
      " [0.0564995 ]\n",
      " [0.05623814]\n",
      " [0.05625567]\n",
      " [0.05639565]\n",
      " [0.05631219]\n",
      " [0.05630487]\n",
      " [0.05640193]\n",
      " [0.05624842]\n",
      " [0.05634064]\n",
      " [0.05626457]\n",
      " [0.05626368]\n",
      " [0.05626171]\n",
      " [0.05639745]\n",
      " [0.05638101]\n",
      " [0.05623174]\n",
      " [0.05644983]\n",
      " [0.05629535]\n",
      " [0.05653926]\n",
      " [0.05636017]\n",
      " [0.05641338]\n",
      " [0.05622053]\n",
      " [0.0562473 ]\n",
      " [0.05629327]\n",
      " [0.0565556 ]\n",
      " [0.05627548]\n",
      " [0.05656391]\n",
      " [0.05635633]\n",
      " [0.05624698]\n",
      " [0.05671528]\n",
      " [0.05640333]\n",
      " [0.05640047]\n",
      " [0.05623394]\n",
      " [0.05634723]\n",
      " [0.05634533]\n",
      " [0.05626993]\n",
      " [0.0564497 ]\n",
      " [0.0563996 ]\n",
      " [0.05635375]\n",
      " [0.05627044]\n",
      " [0.05629298]\n",
      " [0.05644424]\n",
      " [0.05649456]\n",
      " [0.05640534]\n",
      " [0.05641076]\n",
      " [0.05622869]\n",
      " [0.05644461]\n",
      " [0.05642582]\n",
      " [0.05626144]\n",
      " [0.05630776]\n",
      " [0.05646056]\n",
      " [0.05626154]\n",
      " [0.0563314 ]\n",
      " [0.05644825]\n",
      " [0.05634154]\n",
      " [0.05627764]\n",
      " [0.05645325]\n",
      " [0.05636902]\n",
      " [0.05641641]\n",
      " [0.05646494]\n",
      " [0.05638914]\n",
      " [0.05625426]\n",
      " [0.0563309 ]\n",
      " [0.0562269 ]\n",
      " [0.05621935]\n",
      " [0.05627191]\n",
      " [0.05631038]\n",
      " [0.05651552]\n",
      " [0.05624018]\n",
      " [0.05631795]\n",
      " [0.05645458]\n",
      " [0.0563314 ]\n",
      " [0.05638393]\n",
      " [0.05655189]\n",
      " [0.05641488]\n",
      " [0.05643407]\n",
      " [0.05623595]\n",
      " [0.05655254]\n",
      " [0.05628459]\n",
      " [0.05640783]\n",
      " [0.05624222]\n",
      " [0.056555  ]\n",
      " [0.05632606]\n",
      " [0.05622094]\n",
      " [0.0566596 ]\n",
      " [0.05644123]\n",
      " [0.05624092]\n",
      " [0.05636217]\n",
      " [0.05644564]\n",
      " [0.05635131]\n",
      " [0.05668052]\n",
      " [0.05632172]\n",
      " [0.05627953]\n",
      " [0.05625334]\n",
      " [0.05635829]\n",
      " [0.05626744]\n",
      " [0.0566383 ]\n",
      " [0.05630932]\n",
      " [0.05626346]\n",
      " [0.05649193]\n",
      " [0.05636781]\n",
      " [0.05619601]\n",
      " [0.05620416]\n",
      " [0.05640478]\n",
      " [0.0564171 ]\n",
      " [0.05627992]\n",
      " [0.05625837]\n",
      " [0.05630594]\n",
      " [0.05644578]\n",
      " [0.05628043]\n",
      " [0.05643035]\n",
      " [0.05627354]\n",
      " [0.05622608]\n",
      " [0.05625878]\n",
      " [0.0562328 ]\n",
      " [0.05636865]\n",
      " [0.05628909]\n",
      " [0.05627099]\n",
      " [0.05621402]\n",
      " [0.05636127]\n",
      " [0.05642489]\n",
      " [0.056301  ]\n",
      " [0.0566314 ]\n",
      " [0.05637798]\n",
      " [0.05643795]\n",
      " [0.05636749]\n",
      " [0.05637538]\n",
      " [0.05628172]\n",
      " [0.05630019]\n",
      " [0.05640768]\n",
      " [0.05627225]\n",
      " [0.05630654]\n",
      " [0.0562469 ]\n",
      " [0.05630644]\n",
      " [0.05643687]\n",
      " [0.05641474]\n",
      " [0.05623641]\n",
      " [0.05644384]\n",
      " [0.05648195]\n",
      " [0.05630683]\n",
      " [0.05642313]\n",
      " [0.05635769]\n",
      " [0.05620179]\n",
      " [0.05661787]\n",
      " [0.0562831 ]\n",
      " [0.05640374]\n",
      " [0.05637448]\n",
      " [0.056327  ]\n",
      " [0.05651971]\n",
      " [0.05640501]\n",
      " [0.05622356]\n",
      " [0.05624681]\n",
      " [0.05642442]\n",
      " [0.05653387]\n",
      " [0.05658142]\n",
      " [0.05631116]\n",
      " [0.05625712]\n",
      " [0.05628859]\n",
      " [0.05637597]\n",
      " [0.05633702]\n",
      " [0.05660897]\n",
      " [0.05630441]\n",
      " [0.05644499]\n",
      " [0.05628708]\n",
      " [0.05634774]\n",
      " [0.05642002]\n",
      " [0.05627256]\n",
      " [0.05626687]\n",
      " [0.05629575]\n",
      " [0.05620833]\n",
      " [0.05651086]\n",
      " [0.0568928 ]\n",
      " [0.05627523]\n",
      " [0.05624796]\n",
      " [0.0565342 ]\n",
      " [0.056324  ]\n",
      " [0.05663507]\n",
      " [0.05677361]\n",
      " [0.0563852 ]\n",
      " [0.05652589]\n",
      " [0.05629971]\n",
      " [0.05625704]\n",
      " [0.05644128]\n",
      " [0.05626193]\n",
      " [0.05629614]\n",
      " [0.05622658]\n",
      " [0.05629494]\n",
      " [0.05621035]\n",
      " [0.05625382]\n",
      " [0.05657187]\n",
      " [0.05627451]\n",
      " [0.05639013]\n",
      " [0.05624307]\n",
      " [0.05646422]\n",
      " [0.05621448]\n",
      " [0.05634084]\n",
      " [0.05639645]\n",
      " [0.05675084]\n",
      " [0.05625443]\n",
      " [0.05626614]\n",
      " [0.0562739 ]\n",
      " [0.05631314]\n",
      " [0.05667984]\n",
      " [0.05636003]\n",
      " [0.05625045]\n",
      " [0.05661506]\n",
      " [0.05619666]\n",
      " [0.05649047]\n",
      " [0.05643705]\n",
      " [0.05654686]\n",
      " [0.05634497]\n",
      " [0.056337  ]\n",
      " [0.05647884]\n",
      " [0.05635518]\n",
      " [0.05641226]\n",
      " [0.05650273]\n",
      " [0.05621043]\n",
      " [0.05645677]\n",
      " [0.05650476]\n",
      " [0.05653577]\n",
      " [0.05627681]\n",
      " [0.05643998]\n",
      " [0.05655546]\n",
      " [0.05639501]\n",
      " [0.05626774]\n",
      " [0.0563052 ]\n",
      " [0.05628022]\n",
      " [0.05647254]\n",
      " [0.0565019 ]\n",
      " [0.05625018]\n",
      " [0.05630033]\n",
      " [0.0567513 ]\n",
      " [0.05636481]\n",
      " [0.05633155]\n",
      " [0.05632681]\n",
      " [0.0563651 ]\n",
      " [0.05656974]\n",
      " [0.05619682]\n",
      " [0.05653465]\n",
      " [0.05640768]\n",
      " [0.05629294]\n",
      " [0.05624294]\n",
      " [0.05640906]\n",
      " [0.05625725]\n",
      " [0.05635915]\n",
      " [0.05628377]\n",
      " [0.05636013]\n",
      " [0.05630345]\n",
      " [0.05633889]\n",
      " [0.05626603]\n",
      " [0.05644499]\n",
      " [0.0564163 ]\n",
      " [0.05639562]\n",
      " [0.05650336]\n",
      " [0.0562515 ]\n",
      " [0.05634715]\n",
      " [0.05643777]\n",
      " [0.05635723]\n",
      " [0.05656769]\n",
      " [0.05632497]\n",
      " [0.05635038]\n",
      " [0.05641457]\n",
      " [0.05643746]\n",
      " [0.05639052]\n",
      " [0.05629803]\n",
      " [0.05645801]\n",
      " [0.05648745]\n",
      " [0.05624996]\n",
      " [0.05654106]\n",
      " [0.05641006]\n",
      " [0.05630026]\n",
      " [0.05627807]\n",
      " [0.05644004]\n",
      " [0.05627171]\n",
      " [0.0562844 ]\n",
      " [0.05632818]\n",
      " [0.05656239]\n",
      " [0.05627052]\n",
      " [0.05640684]\n",
      " [0.05626062]\n",
      " [0.05629717]\n",
      " [0.05630472]\n",
      " [0.05679522]\n",
      " [0.05633283]\n",
      " [0.05644266]\n",
      " [0.05633328]\n",
      " [0.05645845]\n",
      " [0.0565574 ]\n",
      " [0.05654159]\n",
      " [0.05662795]\n",
      " [0.05631209]\n",
      " [0.05624337]\n",
      " [0.05641808]\n",
      " [0.05626171]\n",
      " [0.05626677]\n",
      " [0.05637584]\n",
      " [0.05638578]\n",
      " [0.0565088 ]\n",
      " [0.05626635]\n",
      " [0.05626757]\n",
      " [0.05645574]\n",
      " [0.05629808]\n",
      " [0.05631601]\n",
      " [0.05628365]\n",
      " [0.05627304]\n",
      " [0.05636774]\n",
      " [0.05626795]\n",
      " [0.05628636]\n",
      " [0.05623553]\n",
      " [0.05626316]\n",
      " [0.05625723]\n",
      " [0.0565233 ]\n",
      " [0.05633567]\n",
      " [0.05634539]\n",
      " [0.05630268]\n",
      " [0.05635345]\n",
      " [0.0562482 ]\n",
      " [0.05648186]\n",
      " [0.05626727]\n",
      " [0.05632032]\n",
      " [0.05639418]\n",
      " [0.05627523]\n",
      " [0.05643971]\n",
      " [0.05659462]\n",
      " [0.05644106]\n",
      " [0.05667447]\n",
      " [0.0565453 ]\n",
      " [0.05619421]\n",
      " [0.05619548]\n",
      " [0.05629725]\n",
      " [0.05652723]\n",
      " [0.05644045]\n",
      " [0.05622044]\n",
      " [0.05654325]\n",
      " [0.05630624]\n",
      " [0.05627767]\n",
      " [0.05627253]\n",
      " [0.05649047]\n",
      " [0.05628637]\n",
      " [0.05629105]\n",
      " [0.05648155]\n",
      " [0.05639962]\n",
      " [0.05642048]\n",
      " [0.05660119]\n",
      " [0.05627368]\n",
      " [0.05646405]\n",
      " [0.05633646]\n",
      " [0.05622607]\n",
      " [0.0567137 ]\n",
      " [0.05639233]\n",
      " [0.05623672]\n",
      " [0.05671505]\n",
      " [0.05626704]\n",
      " [0.05626436]\n",
      " [0.05627583]\n",
      " [0.05633101]\n",
      " [0.0563835 ]\n",
      " [0.05629911]\n",
      " [0.05626159]\n",
      " [0.05626344]\n",
      " [0.05637515]\n",
      " [0.05654003]\n",
      " [0.0563106 ]\n",
      " [0.05646237]\n",
      " [0.05657183]\n",
      " [0.0562374 ]\n",
      " [0.05656698]\n",
      " [0.05635807]\n",
      " [0.05635642]\n",
      " [0.05632381]\n",
      " [0.05645812]\n",
      " [0.05625878]\n",
      " [0.05632817]\n",
      " [0.05652129]\n",
      " [0.05647475]\n",
      " [0.05636912]\n",
      " [0.05628784]\n",
      " [0.05635237]\n",
      " [0.05659597]\n",
      " [0.05621308]\n",
      " [0.05625453]\n",
      " [0.05631341]\n",
      " [0.05632714]\n",
      " [0.05638932]\n",
      " [0.05627482]\n",
      " [0.05652282]\n",
      " [0.05626181]\n",
      " [0.05619976]\n",
      " [0.0562672 ]\n",
      " [0.05651902]\n",
      " [0.05626494]\n",
      " [0.05635406]\n",
      " [0.05626149]\n",
      " [0.05627325]\n",
      " [0.05631866]\n",
      " [0.05631588]\n",
      " [0.05623642]\n",
      " [0.05655929]\n",
      " [0.05639647]\n",
      " [0.05630708]\n",
      " [0.05650815]\n",
      " [0.05629573]\n",
      " [0.05654183]\n",
      " [0.05656398]\n",
      " [0.05629148]\n",
      " [0.05657012]\n",
      " [0.05628327]\n",
      " [0.05629569]\n",
      " [0.05656994]\n",
      " [0.0562217 ]\n",
      " [0.05629483]\n",
      " [0.05630747]\n",
      " [0.05631686]\n",
      " [0.05627007]\n",
      " [0.05621864]\n",
      " [0.05626417]\n",
      " [0.05644325]\n",
      " [0.0563677 ]\n",
      " [0.05628586]\n",
      " [0.0562707 ]\n",
      " [0.05629017]\n",
      " [0.05629392]\n",
      " [0.05638062]\n",
      " [0.05658343]\n",
      " [0.05645544]\n",
      " [0.05642843]\n",
      " [0.05640192]\n",
      " [0.0562686 ]\n",
      " [0.05625612]\n",
      " [0.05648027]\n",
      " [0.05632241]\n",
      " [0.05624802]\n",
      " [0.05637865]\n",
      " [0.05653386]\n",
      " [0.05629939]\n",
      " [0.05630852]\n",
      " [0.05630276]\n",
      " [0.05643524]\n",
      " [0.05640809]\n",
      " [0.05633806]\n",
      " [0.05627373]\n",
      " [0.05690809]\n",
      " [0.05629405]\n",
      " [0.05641969]\n",
      " [0.05620369]\n",
      " [0.05631904]\n",
      " [0.05655307]\n",
      " [0.0564131 ]\n",
      " [0.05626814]\n",
      " [0.05629545]\n",
      " [0.05624449]\n",
      " [0.0564307 ]\n",
      " [0.0564913 ]\n",
      " [0.05639433]\n",
      " [0.05650183]\n",
      " [0.05641604]\n",
      " [0.05648667]\n",
      " [0.05664921]\n",
      " [0.05638607]\n",
      " [0.05642425]\n",
      " [0.05645768]\n",
      " [0.05644204]\n",
      " [0.05690021]\n",
      " [0.05630018]\n",
      " [0.05643646]\n",
      " [0.05640006]\n",
      " [0.05634641]\n",
      " [0.05619273]\n",
      " [0.05688319]\n",
      " [0.0562475 ]\n",
      " [0.05627256]\n",
      " [0.05642197]\n",
      " [0.05625401]\n",
      " [0.05653513]\n",
      " [0.05657871]\n",
      " [0.05628339]\n",
      " [0.05666877]\n",
      " [0.05633619]\n",
      " [0.05642675]\n",
      " [0.05649415]\n",
      " [0.0562999 ]\n",
      " [0.05640123]\n",
      " [0.0564409 ]\n",
      " [0.05627001]\n",
      " [0.05627134]\n",
      " [0.05639407]\n",
      " [0.05632601]\n",
      " [0.05626762]\n",
      " [0.05642982]\n",
      " [0.05625395]\n",
      " [0.05637527]\n",
      " [0.05660347]\n",
      " [0.05642962]\n",
      " [0.05644522]\n",
      " [0.05627884]\n",
      " [0.05622655]\n",
      " [0.05628167]\n",
      " [0.05634293]\n",
      " [0.05667759]\n",
      " [0.05645993]\n",
      " [0.05641157]\n",
      " [0.05656415]\n",
      " [0.05625373]\n",
      " [0.05628636]\n",
      " [0.05626707]\n",
      " [0.05622057]\n",
      " [0.0562123 ]\n",
      " [0.05629477]\n",
      " [0.05634578]\n",
      " [0.05623035]\n",
      " [0.05628471]\n",
      " [0.05650917]\n",
      " [0.0562264 ]\n",
      " [0.05627764]\n",
      " [0.05638801]\n",
      " [0.05627819]\n",
      " [0.05642324]\n",
      " [0.05687475]\n",
      " [0.05633661]\n",
      " [0.05640735]\n",
      " [0.05637357]\n",
      " [0.05645647]\n",
      " [0.05624915]\n",
      " [0.0562785 ]\n",
      " [0.0564963 ]\n",
      " [0.05653336]\n",
      " [0.05627783]\n",
      " [0.05649441]\n",
      " [0.05635513]\n",
      " [0.0562618 ]\n",
      " [0.05639219]\n",
      " [0.05627098]\n",
      " [0.05642759]\n",
      " [0.05636001]\n",
      " [0.05626164]\n",
      " [0.05631888]\n",
      " [0.05632225]\n",
      " [0.05642468]\n",
      " [0.05651969]\n",
      " [0.05627607]\n",
      " [0.05641033]\n",
      " [0.0563295 ]\n",
      " [0.05656683]\n",
      " [0.05631643]\n",
      " [0.05643652]\n",
      " [0.05637197]\n",
      " [0.05647694]\n",
      " [0.05646817]\n",
      " [0.05653118]\n",
      " [0.05634673]\n",
      " [0.05641964]\n",
      " [0.05646059]\n",
      " [0.05651295]\n",
      " [0.05628647]\n",
      " [0.056303  ]\n",
      " [0.05635403]\n",
      " [0.05629943]\n",
      " [0.05628143]\n",
      " [0.05630292]\n",
      " [0.05633664]\n",
      " [0.05643219]\n",
      " [0.05620744]\n",
      " [0.05650698]\n",
      " [0.05628996]\n",
      " [0.05633833]\n",
      " [0.05633457]\n",
      " [0.05626207]\n",
      " [0.05643288]\n",
      " [0.05647634]\n",
      " [0.05639917]\n",
      " [0.05642918]\n",
      " [0.05640829]\n",
      " [0.0562168 ]\n",
      " [0.05667276]\n",
      " [0.05688455]\n",
      " [0.056537  ]\n",
      " [0.05620915]\n",
      " [0.05632127]\n",
      " [0.05628217]\n",
      " [0.05645757]\n",
      " [0.05624084]\n",
      " [0.05620594]\n",
      " [0.05643131]\n",
      " [0.05630271]\n",
      " [0.05643456]\n",
      " [0.05629376]\n",
      " [0.0564961 ]\n",
      " [0.05639239]\n",
      " [0.05649941]\n",
      " [0.05643173]\n",
      " [0.0563527 ]\n",
      " [0.05644258]\n",
      " [0.05625666]\n",
      " [0.05635438]\n",
      " [0.05629712]\n",
      " [0.05639448]\n",
      " [0.05683491]\n",
      " [0.05636848]\n",
      " [0.05656502]\n",
      " [0.05633556]\n",
      " [0.05649232]\n",
      " [0.05629948]\n",
      " [0.05626997]\n",
      " [0.05642294]\n",
      " [0.05639612]\n",
      " [0.05631712]\n",
      " [0.05646965]\n",
      " [0.05620133]\n",
      " [0.05626008]\n",
      " [0.05678673]\n",
      " [0.0564536 ]\n",
      " [0.05624153]\n",
      " [0.05624422]\n",
      " [0.05624341]\n",
      " [0.05639653]\n",
      " [0.0564039 ]\n",
      " [0.05650929]\n",
      " [0.05675179]\n",
      " [0.05663987]\n",
      " [0.05622861]\n",
      " [0.05627959]\n",
      " [0.05634848]\n",
      " [0.05626389]\n",
      " [0.05636606]\n",
      " [0.05649902]\n",
      " [0.05642337]\n",
      " [0.0562764 ]\n",
      " [0.0562715 ]\n",
      " [0.05622347]\n",
      " [0.05649768]\n",
      " [0.0565759 ]\n",
      " [0.05626092]\n",
      " [0.05639815]\n",
      " [0.05655593]\n",
      " [0.05624466]\n",
      " [0.0564535 ]\n",
      " [0.05624616]\n",
      " [0.05639945]\n",
      " [0.05622628]\n",
      " [0.05637567]\n",
      " [0.05629827]\n",
      " [0.05624062]\n",
      " [0.05643509]\n",
      " [0.05626588]\n",
      " [0.05647863]\n",
      " [0.05641706]\n",
      " [0.05639929]\n",
      " [0.05626578]\n",
      " [0.05638217]\n",
      " [0.05638126]\n",
      " [0.05644592]\n",
      " [0.05624521]\n",
      " [0.0562907 ]\n",
      " [0.05642519]\n",
      " [0.05638275]\n",
      " [0.05648126]\n",
      " [0.05621506]\n",
      " [0.0564735 ]\n",
      " [0.0562513 ]\n",
      " [0.05639654]\n",
      " [0.05623258]\n",
      " [0.05659579]\n",
      " [0.05656543]\n",
      " [0.05649342]\n",
      " [0.05688182]\n",
      " [0.05643716]\n",
      " [0.05644976]\n",
      " [0.05623779]\n",
      " [0.05649205]\n",
      " [0.05641976]\n",
      " [0.05625432]\n",
      " [0.05662977]\n",
      " [0.05644125]\n",
      " [0.05623772]\n",
      " [0.05687934]\n",
      " [0.05637311]\n",
      " [0.0563989 ]\n",
      " [0.05689044]\n",
      " [0.05652011]\n",
      " [0.05646884]\n",
      " [0.05633392]\n",
      " [0.05639009]\n",
      " [0.05621665]\n",
      " [0.05643233]\n",
      " [0.05625192]\n",
      " [0.05627212]\n",
      " [0.05628335]\n",
      " [0.05635302]\n",
      " [0.0562527 ]\n",
      " [0.05643674]\n",
      " [0.056459  ]\n",
      " [0.05661225]\n",
      " [0.05625562]\n",
      " [0.05655053]\n",
      " [0.05623074]\n",
      " [0.05650602]\n",
      " [0.05634712]\n",
      " [0.05627112]\n",
      " [0.0562847 ]\n",
      " [0.05622325]\n",
      " [0.05661676]\n",
      " [0.05619648]\n",
      " [0.05635752]\n",
      " [0.05657498]\n",
      " [0.05652556]\n",
      " [0.05626706]\n",
      " [0.05627217]\n",
      " [0.05628522]\n",
      " [0.05643402]\n",
      " [0.05641621]\n",
      " [0.05636876]\n",
      " [0.05630572]\n",
      " [0.05679519]\n",
      " [0.0564493 ]\n",
      " [0.05639723]\n",
      " [0.05636325]\n",
      " [0.05631753]\n",
      " [0.056411  ]\n",
      " [0.05655665]\n",
      " [0.05652389]\n",
      " [0.05638045]\n",
      " [0.0564026 ]\n",
      " [0.05622992]\n",
      " [0.05623523]\n",
      " [0.05627776]\n",
      " [0.05627122]\n",
      " [0.05649361]\n",
      " [0.05643123]\n",
      " [0.05677035]\n",
      " [0.05626912]\n",
      " [0.05626445]\n",
      " [0.05624768]\n",
      " [0.05646909]\n",
      " [0.05637693]\n",
      " [0.05626616]\n",
      " [0.05628602]\n",
      " [0.05625336]\n",
      " [0.05632472]\n",
      " [0.05624471]\n",
      " [0.05630767]\n",
      " [0.05625591]\n",
      " [0.05625582]\n",
      " [0.05629981]\n",
      " [0.05633728]\n",
      " [0.05625089]\n",
      " [0.05640672]\n",
      " [0.0564056 ]\n",
      " [0.05664478]\n",
      " [0.05635349]\n",
      " [0.05632246]\n",
      " [0.05626772]\n",
      " [0.05627093]\n",
      " [0.05620348]\n",
      " [0.05622003]\n",
      " [0.05641251]\n",
      " [0.05623727]\n",
      " [0.05631345]\n",
      " [0.05651664]\n",
      " [0.05627527]\n",
      " [0.05622413]\n",
      " [0.05639251]\n",
      " [0.05626357]\n",
      " [0.05630608]\n",
      " [0.0564054 ]\n",
      " [0.05624257]\n",
      " [0.05631137]\n",
      " [0.05658262]\n",
      " [0.05645268]\n",
      " [0.05654533]\n",
      " [0.05642133]\n",
      " [0.05628251]\n",
      " [0.056417  ]\n",
      " [0.05628408]\n",
      " [0.05655414]\n",
      " [0.05646764]\n",
      " [0.05669798]\n",
      " [0.05629021]\n",
      " [0.05631635]\n",
      " [0.05649712]\n",
      " [0.05638558]\n",
      " [0.0563375 ]\n",
      " [0.05680393]\n",
      " [0.05653495]\n",
      " [0.05642854]\n",
      " [0.05625089]\n",
      " [0.05657794]\n",
      " [0.05678308]\n",
      " [0.05649457]\n",
      " [0.05657976]\n",
      " [0.05652893]\n",
      " [0.05655138]\n",
      " [0.05620928]\n",
      " [0.05626236]\n",
      " [0.05640073]\n",
      " [0.05650872]\n",
      " [0.05683903]\n",
      " [0.0562349 ]\n",
      " [0.05649967]\n",
      " [0.05644403]\n",
      " [0.05680624]\n",
      " [0.05626505]\n",
      " [0.05647565]\n",
      " [0.05635544]\n",
      " [0.05637183]\n",
      " [0.05627784]\n",
      " [0.05630248]\n",
      " [0.05628407]\n",
      " [0.05644662]\n",
      " [0.05645431]\n",
      " [0.05631825]\n",
      " [0.0564247 ]\n",
      " [0.05644567]\n",
      " [0.05630364]\n",
      " [0.05636139]\n",
      " [0.05637258]\n",
      " [0.05630057]\n",
      " [0.05656845]\n",
      " [0.05632906]\n",
      " [0.05631993]\n",
      " [0.05623574]\n",
      " [0.05656134]\n",
      " [0.05627257]\n",
      " [0.05649921]\n",
      " [0.05625255]\n",
      " [0.05627982]\n",
      " [0.05629846]\n",
      " [0.05648614]\n",
      " [0.05630235]\n",
      " [0.05629142]\n",
      " [0.05637905]\n",
      " [0.0562698 ]\n",
      " [0.05638354]\n",
      " [0.05628138]\n",
      " [0.05642318]\n",
      " [0.05635717]\n",
      " [0.05640446]\n",
      " [0.05626173]\n",
      " [0.05645634]\n",
      " [0.05640704]\n",
      " [0.05661084]\n",
      " [0.05647536]\n",
      " [0.05647231]\n",
      " [0.05642883]\n",
      " [0.05645427]\n",
      " [0.05626886]\n",
      " [0.05623504]\n",
      " [0.05643928]\n",
      " [0.05626881]\n",
      " [0.05627833]\n",
      " [0.05651167]\n",
      " [0.05647064]\n",
      " [0.05636641]\n",
      " [0.05641545]\n",
      " [0.0564873 ]\n",
      " [0.056274  ]\n",
      " [0.05628292]\n",
      " [0.05651423]\n",
      " [0.05642454]\n",
      " [0.05623786]\n",
      " [0.05628329]\n",
      " [0.05622496]\n",
      " [0.05630043]\n",
      " [0.05626728]\n",
      " [0.0562569 ]\n",
      " [0.05625694]\n",
      " [0.05627562]\n",
      " [0.05631731]\n",
      " [0.05625987]\n",
      " [0.05624352]\n",
      " [0.05654562]\n",
      " [0.05632187]\n",
      " [0.05627088]\n",
      " [0.05631209]\n",
      " [0.05630256]\n",
      " [0.05645927]\n",
      " [0.05627975]\n",
      " [0.05645838]\n",
      " [0.05627776]\n",
      " [0.05627835]\n",
      " [0.05622102]\n",
      " [0.0563766 ]\n",
      " [0.05652842]\n",
      " [0.05628703]\n",
      " [0.05634448]\n",
      " [0.056269  ]\n",
      " [0.05642833]\n",
      " [0.05639965]\n",
      " [0.0564264 ]\n",
      " [0.0563735 ]\n",
      " [0.05630685]\n",
      " [0.05632577]\n",
      " [0.0563153 ]\n",
      " [0.05635735]\n",
      " [0.05627537]\n",
      " [0.0563181 ]\n",
      " [0.05642752]\n",
      " [0.05656119]\n",
      " [0.0563839 ]\n",
      " [0.0562878 ]\n",
      " [0.05622766]\n",
      " [0.05621768]\n",
      " [0.05653197]\n",
      " [0.05628096]\n",
      " [0.05646251]\n",
      " [0.05642192]\n",
      " [0.05635537]\n",
      " [0.05640828]\n",
      " [0.05652844]\n",
      " [0.05633194]\n",
      " [0.05645763]\n",
      " [0.05654453]\n",
      " [0.05626176]\n",
      " [0.05625982]\n",
      " [0.05624364]\n",
      " [0.05639604]\n",
      " [0.05671368]\n",
      " [0.05648886]\n",
      " [0.05651414]\n",
      " [0.05633965]\n",
      " [0.05639268]\n",
      " [0.05635946]\n",
      " [0.05625626]\n",
      " [0.05631043]\n",
      " [0.05637826]\n",
      " [0.05623773]\n",
      " [0.05622362]\n",
      " [0.05643155]\n",
      " [0.05627916]\n",
      " [0.05625841]\n",
      " [0.05641694]\n",
      " [0.05626329]\n",
      " [0.05625487]\n",
      " [0.05654097]\n",
      " [0.05626306]\n",
      " [0.05631158]\n",
      " [0.05644156]\n",
      " [0.05629208]\n",
      " [0.05623957]\n",
      " [0.05649688]\n",
      " [0.05628176]\n",
      " [0.05631295]\n",
      " [0.05640451]\n",
      " [0.0563865 ]\n",
      " [0.05637582]\n",
      " [0.05621964]\n",
      " [0.05640037]\n",
      " [0.05635662]\n",
      " [0.05643299]\n",
      " [0.05623456]\n",
      " [0.0562874 ]\n",
      " [0.05632896]\n",
      " [0.05624115]\n",
      " [0.05654893]\n",
      " [0.0563417 ]\n",
      " [0.05625409]\n",
      " [0.0562795 ]\n",
      " [0.05630834]\n",
      " [0.05630837]\n",
      " [0.05631308]\n",
      " [0.05626248]\n",
      " [0.05630301]\n",
      " [0.05640545]\n",
      " [0.05664992]\n",
      " [0.05621085]\n",
      " [0.05627137]\n",
      " [0.05639654]\n",
      " [0.05630048]\n",
      " [0.05645847]\n",
      " [0.056418  ]\n",
      " [0.05640109]\n",
      " [0.0562779 ]\n",
      " [0.05643577]\n",
      " [0.05636781]\n",
      " [0.05620672]\n",
      " [0.05646871]\n",
      " [0.0561872 ]\n",
      " [0.05625802]\n",
      " [0.0561961 ]\n",
      " [0.05623046]\n",
      " [0.05636744]\n",
      " [0.05636712]\n",
      " [0.0563594 ]\n",
      " [0.05639933]\n",
      " [0.05649104]\n",
      " [0.05626127]\n",
      " [0.05629247]\n",
      " [0.05626848]\n",
      " [0.05633022]\n",
      " [0.05639235]\n",
      " [0.05631208]\n",
      " [0.05633847]\n",
      " [0.05620555]\n",
      " [0.05639082]\n",
      " [0.05671786]\n",
      " [0.05639555]\n",
      " [0.05638661]]\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(predictions, Y):\n",
    "    #print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / 500\n",
    "\n",
    "accuracy=get_accuracy(test_predictions_np,data[len(train_data):5000,-1])\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
