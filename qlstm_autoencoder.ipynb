{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits,\n",
    "                n_qlayers=1,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit.torch\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size #features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "\n",
    "        #self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        #self.dev = qml.device('qiskit.basicaer', wires=self.n_qubits)\n",
    "        #self.dev = qml.device('qiskit.ibm', wires=self.n_qubits)\n",
    "        # use 'qiskit.ibmq' instead to run on hardware\n",
    "\n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget,torch_device='cuda')\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input,torch_device='cuda')\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update,torch_device='cuda')\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output,torch_device='cuda')\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_forget)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\",diff_method='backprop')\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_input)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\",diff_method='backprop')\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_update)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\",diff_method='backprop')\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_output)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\",diff_method='backprop')\n",
    "\n",
    "        weight_shapesf = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        weight_shapesi = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        weight_shapesu = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        weight_shapeso = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        device = torch.device('cuda')\n",
    "        \n",
    "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapesf),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapesi),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapesu),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapeso)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        device = torch.device('cuda')\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).to(device)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size).to(device)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :] #x has shape (batch,seq_len,features)\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t) #it has size (batch_size, hidden)\n",
    "            hidden_seq.append(h_t.unsqueeze(0)) #we will end with a number of sequences of the size of the window of time \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0) #(window, batch_size,hidden)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous() #(batch_size,window,hidden)\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, embedding_dim, n_qubits,  backend='default.qubit.torch'):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.input_size=input_size #number of features\n",
    "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "    self.n_qubits=n_qubits\n",
    "    self.rnn1 = QLSTM(self.input_size, self.hidden_dim, self.n_qubits, backend=backend)\n",
    "    \n",
    "    self.rnn2 = QLSTM(self.hidden_dim,embedding_dim,self.n_qubits, backend=backend)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #x = x.reshape((1, self.seq_len, self.n_features))\n",
    "    #print(x.shape)\n",
    "    x, (_, _) = self.rnn1(x)\n",
    "    \n",
    "    \n",
    "    x, (hidden_n, _) = self.rnn2(x)\n",
    "    \n",
    "\n",
    "    #return hidden_n.reshape((self.n_features, self.embedding_dim))\n",
    "    return x\n",
    "  \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, seq_len,n_features,n_qubits, input_dim=64,backend='default.qubit.torch'):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.seq_len, self.input_dim = seq_len, input_dim\n",
    "    self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "    self.n_qubits=n_qubits\n",
    "\n",
    "    self.rnn1 = QLSTM(\n",
    "      self.input_dim,\n",
    "      self.input_dim,\n",
    "      self.n_qubits,\n",
    "      backend=backend\n",
    "    )\n",
    "\n",
    "    self.rnn2 = QLSTM(\n",
    "      self.input_dim,\n",
    "      self.hidden_dim,\n",
    "      self.n_qubits,\n",
    "      backend=backend\n",
    "    )\n",
    "\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    #x = x.repeat(self.seq_len, self.n_features)\n",
    "    #x = x.reshape((self.n_features, self.seq_len, self.input_dim))\n",
    "\n",
    "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "    x = x.reshape((self.seq_len, self.hidden_dim))\n",
    "\n",
    "    return self.output_layer(x)\n",
    "  \n",
    "\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, n_qubits,embedding_dim=64):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "        self.decoder = Decoder(seq_len,  n_features,n_qubits).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
